0.8425 : 37. Loh WY. Logistic regression tree analysis. In: PhamH, ed. Handbook of Engineering Statistics. London:Springer; 2006, 537–549.
0.775 : Figure 10: treefit2.pruned’s partition of California. Compare to Figure 5.
0.76 : TABLE 2 Predictor Variables for the Car Data
0.71 : the help of Figures 6 and 7, which plot the data val-ues of FEV and ht and the fitted regression functionswith a different symbol and color for each node. InFigure 6, the slope of ht is zero for the group offemales taller than 66.2 in., but it is constant andnonzero across the other groups. This indicates athree-way interaction involving age, ht, and sex. Asimilar conclusion can be drawn from Figure 7, inwhich there are only three groups, with the group ofchildren aged 11 years or below exhibiting a quadraticeffect of ht on FEV. For children above the age of 11years, the effect of ht is linear, but males have, on av-erage, about a half liter more FEV than females. Thus,the effect of sex seems to be due mainly to childrenolder than 11 years. This conclusion is reinforced bythe piecewise stepwise linear tree in Figure 5(d), inwhich a stepwise linear model is fitted to each of thetwo leaf nodes. Age and ht are selected as linear pre-dictors in both leaf nodes, but sex is selected only inthe node corresponding to children taller than 66.5in., 70% of whom are above 11 years old.
0.7 : reduces the number of searches to just 30 splits). C4.5is not similarly affected because it does not search forbinary splits on unordered X variables. Instead, C4.5splits the node into one branch for each X value andthen merges some branches after the tree is grown.CRUISE, GUIDE, and QUEST also are unaffected be-cause they search exhaustively for splits on unorderedvariables only if the number of values is small. If thelatter is large, these algorithms employ a technique
0.69 : FIGURE 7 | Data and fitted regression functions in the three leaf nodes of the GUIDE piecewise simple quadratic model in Figure 5(c).
0.68 : Berk, Richard A. (2008). Statistical Learning from a Regression Perspective.Springer Series in Statistics. New York: Springer-Verlag.
0.68 : linear regression tree models typically have higherprediction accuracy than piecewise constant models.Empirical results29 show that the accuracy of thepiecewise linear trees can be comparable to thatof spline-based methods and ensembles of piecewiseconstant trees.
0.68 : Prediction TreesRegression TreesExample: California Real Estate AgainRegression Tree FittingCross-Validation and Pruning in R
0.675 : 22. Quinlan JR. Learning with continuous classes. Pro-ceedings of the 5th Australian Joint Conference on Ar-tificial Intelligence 1992, 343–348.
0.67 : E. Alpaydin. Introduction to Machine Learning. 2nd ed. Boston: MIT Press; 2010.
0.66 : 2.2 Regression Tree Fitting
0.65 : There are lots of other cross-validation tricks for trees. One cute one is toalternate growing and pruning. We divide the data into two parts, as before, andfirst grow and then prune the tree. We then exchange the role of the trainingand testing sets, and try to grow our pruned tree to fit the second half. We thenprune again, on the first half. We keep alternating in this manner until the sizeof the tree doesn’t change.
0.65 : using only the data points in that leaf (and using dummy variables for non-quantitativefeatures). This would give a piecewise-linear model, rather than a piecewise-constant one.If we’ve built the tree well, however, all the points in each leaf are pretty similar, so theregression surface would be nearly constant anyway.
0.6425 : 28. Lim TS, Loh WY, Shih YS. A comparison of predictionaccuracy, complexity, and training time of thirty-threeold and new classification algorithms. Machine Learn-ing 2000, 40:203–228.
0.635 : 27. Loh WY. Regression by parts: fitting visually inter-pretable models with GUIDE. In: Chen C, Hordle W,Unwin A, eds. Handbook of Data Visualization. NewYork: Springer; 2008, 447–469.
0.63 : 3.1 Measuring Information
0.6275 : 7. R Development Core Team, R: a language and envi-ronment for statistical computing. R Foundation forStatistical Computing, Vienna, Austria, 2009.
0.6258333333333334 : 15. Kass GV. An exploratory technique for investigatinglarge quantities of categorical data. Appl Stat 1980,29:119–127.
0.6175 : of the predictor variables. For more examples, seeRefs 26 and 27.