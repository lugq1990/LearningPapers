0.9709470202887462 : manuf Manufacturer (31 values) rev Engine revolutions per miletype Type (small, sporty, compact, midsize, large, van) manual Manual transmission available (yes, no)minprice Minimum price (in $1000) fuel Fuel tank capacity (gallons)midprice Midrange price (in $1000) passngr Passenger capacity (persons)maxprice Maximum price (in $1000) length Length (inches)citympg City miles per gallon whlbase Wheelbase (inches)hwympg Highway miles per gallon width Width (inches)airbag Air bags standard (0, 1, 2) uturn U-turn space (feet)cylin Number of cylinders rseat Rear seat room (inches)enginzs Engine size (liters) luggage Luggage capacity (cu. ft.)hp Maximum horsepower weight Weight (pounds)rpm Revolutions per minute at maximum horsepower domestic Domestic (U.S./non-U.S.manufacturer)
0.9600537934890057 : a dominant X variable. Variable passngr is chosenby three algorithms (C4.5, GUIDE QUEST); enginsz,fuel, length, and minprice by two; and hp, hwympg,luggage, maxprice, rev, type, and width by one each.Variables airbag, citympg, cylin, midprice, and rpmare not selected by any.
0.9504810588079365 : To see how the algorithms perform in a real ap-plication, we apply them to a data set on new carsfor the 1993 model year.18 There are 93 cars and 25variables. We let the Y variable be the type of drivetrain, which takes three values (rear, front, or four-wheel drive). The X variables are listed in Table 2.Three are unordered (manuf, type, and airbag, tak-ing 31, 6, and 3 values, respectively), two binary-valued (manual and domestic), and the rest ordered.The class frequencies are rather unequal: 16 (17.2%)are rear, 67 (72.0%) are front, and 10 (10.8%)are four-wheel drive vehicles. To avoid random-ness due to 10-fold cross-validation, we use leave-one-out (i.e., n-fold) cross-validation to prune theCRUISE, GUIDE, QUEST, and RPART trees in thisarticle.
0.9381898011361404 : FIGURE 4 | CRUISE, C4.5, and GUIDE trees for car data with manufincluded. The CHAID, RPART, and QUEST trees are trivial with no splits.Sets S1 and S2 are (Plymouth, Subaru) and (Lexus, Lincoln, Mercedes,Mercury, Volvo), respectively, and S3 is the complement of S1 ∪ S2.
0.9096158613852553 : 22. Quinlan JR. Learning with continuous classes. Pro-ceedings of the 5th Australian Joint Conference on Ar-tificial Intelligence 1992, 343–348.
0.8822918845490763 : Figure 4: As Figure 2, but allowing splits for smaller reductions in error.
0.8801597887766028 : a probability weighting scheme, QUEST imputes themissing values locally, and GUIDE treats missing val-ues as belonging to a separate category. All exceptC4.5 accept user-specified misclassification costs andall except C4.5 and CHAID accept user-specified classprior probabilities. By default, all algorithms fit a con-stant model to each node, predicting Y to be the classwith the smallest misclassification cost. CRUISE canoptionally fit bivariate linear discriminant models andGUIDE can fit bivariate kernel density and nearestneighbor models in the nodes. GUIDE also can pro-duce ensemble models using bagging16 and randomforest17 techniques. Table 1 summarizes the featuresof the algorithms.
0.862469030978942 : # error on training dataprune.tree(my.tree,best=5,newdata=test.set) # Ditto, but evaluates on test.setmy.tree.seq = prune.tree(my.tree) # Sequence of pruned tree sizes/errorsplot(my.tree.seq) # Plots size vs. errormy.tree.seq$dev # Vector of error rates for prunings, in orderopt.trees = which(my.tree.seq$dev == min(my.tree.seq$dev)) # Positions of# optimal (with respect to error) trees
0.8569930484604361 : FIGURE 7 | Data and fitted regression functions in the three leaf nodes of the GUIDE piecewise simple quadratic model in Figure 5(c).
0.8385941050434794 : The tree package contains functions prune.tree and cv.tree for pruning treesby cross-validation.
0.8188646373912726 : 13. Loh WY, Shih Y. Split selection methods for classifica-tion trees. Stat Sin 1997, 7:815–840.
0.8060738312389378 : 7. R Development Core Team, R: a language and envi-ronment for statistical computing. R Foundation forStatistical Computing, Vienna, Austria, 2009.
0.8050822963515514 : Ripley, Brian D. (1996). Pattern Recognition and Neural Networks. Cambridge,England: Cambridge University Press.
0.8032204506051197 : FIGURE 6 | Data and fitted regression lines in the five leaf nodesof the GUIDE piecewise simple linear model in Figure 5(b).
0.8022154074104102 : min(my.tree.seq$size[opt.trees]) # Size of smallest optimal tree
0.7936077947943949 : FIGURE 3 | Data and split at the node marked with an asterisk (*) in the GUIDE tree in Figure 2.
0.7857097956741871 : • There are fast, reliable algorithms to learn these trees
0.7846185579611192 : E. Alpaydin. Introduction to Machine Learning. 2nd ed. Boston: MIT Press; 2010.
0.7772362077397144 : 23. Loh WY. Regression trees with unbiased variable selec-tion and interaction detection. Stat Sin 2002, 12:361–386.
0.7761132520548915 : Age≤ 11 yearsAge> 11 years, femaleAge> 11 years, male